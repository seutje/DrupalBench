import json
import math
import sys
import os
import subprocess

def comb(n, k):
    """Calculate combinations nCr."""
    if k < 0 or k > n:
        return 0
    if k == 0 or k == n:
        return 1
    if k > n // 2:
        k = n - k
    
    numerator = 1
    for i in range(k):
        numerator = numerator * (n - i) // (i + 1)
    return numerator

def calculate_pass_at_k(n, c, k):
    """
    Calculates pass@k metric.
    n: total number of samples
    c: number of correct samples
    k: k value
    """
    if n - c < k:
        return 1.0
    return 1.0 - (comb(n - c, k) / comb(n, k))

def run_phpcs(target_path):
    """Run phpcs with DrupalPractice ruleset."""
    # Ensure path is relative to /var/www/html in container
    container_path = target_path
    if container_path.startswith("/home/seutje/projects/DrupalBench/app/"):
        container_path = container_path.replace("/home/seutje/projects/DrupalBench/app/", "")
    
    try:
        cmd = [
            "docker-compose", "exec", "-T", "drupal", 
            "./vendor/bin/phpcs", "--standard=DrupalPractice", 
            "--report=json",
            container_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if not result.stdout.strip():
            return {"totals": {"errors": 0, "warnings": 0}}
        return json.loads(result.stdout)
    except Exception as e:
        return {"error": str(e)}

def run_phpstan(target_path):
    """Run phpstan at Level 5."""
    container_path = target_path
    if container_path.startswith("/home/seutje/projects/DrupalBench/app/"):
        container_path = container_path.replace("/home/seutje/projects/DrupalBench/app/", "")

    try:
        cmd = [
            "docker-compose", "exec", "-T", "drupal", 
            "./vendor/bin/phpstan", "analyze", "-l", "5", 
            "--error-format=json", "--no-progress",
            container_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if not result.stdout.strip():
             return {"totals": {"errors": 0}}
        return json.loads(result.stdout)
    except Exception as e:
        return {"error": str(e)}

def run_domain_validators(target_path):
    """Run all domain validators from scripts/validators."""
    results = {}
    validators_dir = "scripts/validators"
    if not os.path.exists(validators_dir):
        return results
    
    for validator in os.listdir(validators_dir):
        if validator.endswith("_validator.py"):
            name = validator.replace("_validator.py", "")
            cmd = [sys.executable, os.path.join(validators_dir, validator), target_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            results[name] = {
                "passed": result.returncode == 0,
                "output": result.stdout + result.stderr
            }
    return results

def generate_report(results, output_file="report.md"):
    """Generate Markdown report."""
    with open(output_file, 'w') as f:
        f.write("# DrupalBench Evaluation Report\n\n")
        
        f.write("## Summary Metrics\n")
        n = results.get('total_samples', 1)
        c = results.get('total_correct', 0)
        
        f.write(f"- **Total Tasks:** {results.get('total_tasks', 0)}\n")
        f.write(f"- **Total Samples (n):** {n}\n")
        f.write(f"- **Total Correct (c):** {c}\n")
        
        f.write("\n### pass@k calculation\n")
        f.write("The pass@k metric is calculated as:\n")
        f.write("$$pass@k = 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}$$\n\n")
        
        for k in [1, 5, 10]:
            if n >= k:
                p_k = calculate_pass_at_k(n, c, k)
                f.write(f"- **pass@{k}:** {p_k:.4f}\n")
        
        f.write("\n## Task Details\n")
        f.write("| Task ID | Title | Status | Code Quality |\n")
        f.write("| --- | --- | --- | --- |\n")
        
        for task in results.get('tasks', []):
            status = "✅ Pass" if task.get('passed') else "❌ Fail"
            quality = task.get('quality_summary', 'N/A')
            domain_status = ""
            if 'domain_results' in task:
                for domain, res in task['domain_results'].items():
                    icon = "S" if res.get('passed') else "F"
                    domain_status += f"{domain[0].upper()}:{icon} "
            
            f.write(f"| {task.get('task_id')} | {task.get('title')} | {status} | {quality} {domain_status} |\n")
            
        f.write("\n\n---\nGenerated by DrupalBench Scoring Engine")

if __name__ == "__main__":
    # Example usage / placeholder for integration
    if len(sys.argv) < 2:
        print("Usage: python scoring_engine.py <results.json>")
        sys.exit(1)
        
    results_path = sys.argv[1]
    if not os.path.exists(results_path):
        print(f"Results file not found: {results_path}")
        sys.exit(1)
        
    with open(results_path, 'r') as f:
        results_data = json.load(f)
    
    # In a real run, we would have ran phpcs/phpstan during evaluation 
    # or we can run them now if the results_data points to paths.
    
    generate_report(results_data)
    print(f"Report generated: report.md")
